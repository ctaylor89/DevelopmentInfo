Currency_
Error_Handling
Temporary_Tables_Variables
TSQL_Performance_Tunning

http://msftdbprodsamples.codeplex.com/ - To get sample databases.
http://msdn.microsoft.com/en-us/library/ms124894(v=sql.100).aspx - about the AdventureWorks schemas
http://msdn.microsoft.com/en-us/library/ms167593.aspx SQL Server tutorials. Reporting services etc.
http://social.msdn.microsoft.com/Forums/sqlserver/en-us/home?forum=sqlserversamples - forum
http://www.codeproject.com/Articles/690340/Querying-SQL-Server-2012-Part-I - A good tutorial from code project
http://www.codeproject.com/Articles/29312/Grouping-Sets-in-T-SQL-SQL-Server - A good tutorial from code project
http://www.codeproject.com/Articles/566542/Date-and-Time-Data-Types-and-Functions-SQL-Server - A good tutorial from code project
http://www.infoworld.com/article/2604472/database/10-more-dos-and-donts-for-faster-sql-queries.html


TOOLS
http://www.johnsansom.com/top-5-free-sql-server-tools/
http://sqlsentry.net/plan-explorer/sql-server-query-view.asp  -- SQL Sentry Plan Explorer
http://www.brentozar.com/archive/2011/02/my-favorite-free-sql-server-downloads  -- Tunning tools
http://www.datamanipulation.net/sqlquerystress/  -- SQLQueryStress is a free tool for SQL Server programmers

The GO command isn't a Transact-SQL statement, but a special command recognized by several MS utilities including 
SQL Server Management Studio code editor. The GO command is used to group SQL commands into batches which are sent 
to the server together. The commands included in the batch, that is, the set of commands since the last GO command 
or the start of the session, must be logically consistent. For example, you can't define a variable in one batch 
and then use it in another since the scope of the variable is limited to the batch in which it's defined.
_______________

The clause WITH TIES can be used only with the clauses TOP and ORDER BY. Let us understand from one simple example 
how this clause actually works. Suppose we have 100 rows in the table and out of that 50 rows have same value in the
column which is used in ORDER BY; when you use TOP 10 rows, it will return you only 10 rows, but if you use TOP 10 
WITH TIES, it will return you all the rows that have same value as that of the last record of top 10 — which means 
a total of 50 records.
_______________

A table can have multiple candidate keys. Each candidate key is a column or set of columns that are UNIQUE when taken 
together, and also NOT NULL. Thus, specifying values for all the columns of any candidate key is enough to determine 
that there is one row that meets the criteria, or no rows at all.

Candidate keys are a fundamental concept in the relational data model. It's common practice, if multiple keys are 
present in one table, to designate one of the candidate keys as the primary key. It's also common practice to cause 
any foreign keys to the table to reference the primary key, rather than any other candidate key.
_______________

You can only have one primary key, but you can have multiple columns in your primary key(Composite Primary Key).
You can also have Unique Indexes on your table, which will work a bit like a primary key in that they will enforce 
unique values, and will speed up querying of those values. The DEFAULT constraint is used to insert a default value 
into a column. The default value will be added to all new records, if no other value is specified.

CREATE TABLE Production.TransactionHistoryArchive1  
(  
   TransactionID int NOT NULL,  
   CONSTRAINT PK_TransactionHistoryArchive_TransactionID PRIMARY KEY CLUSTERED (TransactionID)  
);  
GO 
_______________

The Primary key, Foreign Key and Default constraint are the 3 main constraints that need to be considered while 
creating tables or even after creating them. 

Primary Key Constraint: Primary Keys constraints prevents duplicate values for columns and provides unique 
identifier to each column, as well it creates a clustered index on the columns.

* Record Section

Reasons to use a numeric value versus a string in a primary key:
String comparisons take much longer than numeric comparisons. Using a numeric ID reduces the space used in foreign key tables.

Primary keys must NEVER carry data that is usable outside the database. That is, the only reason for keys to be used is to 
refer to records in the database, not to present the value of the keys to the user. The reason for this is that keys are 
immutable and data is not. Name, gender, age, employee type, employee number, etc. are all changeable and should never be used as a key.

NEWSEQUENTIALID - Creates a GUID that is greater than any GUID previously generated by this function on a specified computer 
since Windows was started. After restarting Windows, the GUID can start again from a lower range, but is still globally unique. 
When a GUID column is used as a row identifier, using NEWSEQUENTIALID can be faster than using the NEWID function. This is 
because the NEWID function causes random activity and uses fewer cached data pages. Using NEWSEQUENTIALID also helps to 
completely fill the data and index pages. If privacy is a concern, do not use this function. It is possible to guess the value 
of the next generated GUID and, therefore, access data associated with that GUID.

NEWSEQUENTIALID() can only be used with DEFAULT constraints on table columns of type uniqueidentifier.
CREATE TABLE myTable (ColumnA uniqueidentifier DEFAULT NEWSEQUENTIALID());

Articles on the reasons to use different types of keys:
http://databases.aspfaq.com/database/what-should-i-choose-for-my-primary-key.html
http://blog.codinghorror.com/primary-keys-ids-versus-guids/

Default constraints are a special case of column defaults. A column default is some value or function that the 
column will take when an INSERT statement doesn't explicitly assign a particular value. In other words, the column 
default is what the column will get as a value by default.
_______________
* Record Section
Indexes are special lookup tables that the database search engine can use to speed up data retrieval. Simply put, 
an index is a pointer to data in a table. An index in a database is very similar to an index in the back of a book.

An index helps speed up SELECT queries and WHERE clauses, but it slows down data input, with UPDATE and INSERT 
statements. Indexes can be created or dropped with no effect on the data.

Creating an index involves the CREATE INDEX statement, which allows you to name the index, to specify the table 
and which column or columns to index, and to indicate whether the index is in ascending or descending order.

Indexes can also be unique, similar to the UNIQUE constraint, in that the index prevents duplicate entries in the 
column or combination of columns on which there's an index.

A single-column index is one that is created based on only one table column. The basic syntax is as follows:

CREATE INDEX index_name
ON table_name (column_name);

Unique Indexes:
Unique indexes are used not only for performance, but also for data integrity. A unique index does not allow any 
duplicate values to be inserted into the table. The basic syntax is as follows:

CREATE UNIQUE INDEX index_name
on table_name (column_name);

A composite index is an index on two or more columns of a table. The basic syntax is as follows:

CREATE INDEX index_name
on table_name (column1, column2);

Whether to create a single-column index or a composite index, take into consideration the column(s) that you may 
use very frequently in a query's WHERE clause as filter conditions.
_______________
* Record Section
There is no difference between Unique Index and Unique Constraint. Even though syntax are different the effect is 
the same. Unique Constraint creates Unique Index to maintain the constraint to prevent duplicate keys. Unique 
Index or Primary Key Index are physical structures that maintain uniqueness over some combination of columns across 
all rows of a table. It is a convenient way to enforce a Unique Constraint for SQL Server.

To DROP a UNIQUE Constraint:
	ALTER TABLE Persons
	DROP CONSTRAINT uc_PersonID
_______________
Explanation of ON [PRIMARY]
When you create a database in Microsoft SQL Server you can have multiple file groups, where storage is created in 
multiple places, directories or disks. Each file group can be named. The PRIMARY file group is the default one, 
which is always created, and so the SQL you've written creates your table ON the PRIMARY file group. This also 
means it's usually useless and can be safely removed from the script".
______________
The following statement:
CONSTRAINT pk_UserID PRIMARY KEY (U_Id)

Is the same as this one:
CONSTRAINT pk_UserID PRIMARY KEY CLUSTERED (U_Id)
You can only have the table data physicality ordered by one of the indexes, and by default that index is the one 
used for the primary key (the primary key unique constraint is always supported by an index).

If you want to leave the order of the table data to be stored according to some other index then you should create 
the primary key with:

CONSTRAINT pk_UserID PRIMARY KEY NONCLUSTERED (U_Id)
And then create the clustered index with:
CREATE CLUSTERED INDEX ix_Email ON Users (Email); 
_______________
UDF - User Defined Functions
Functions are computed values and cannot perform permanent environmental changes to SQL Server (i.e. no INSERT 
or UPDATE statements allowed). A Function can be used inline in SQL Statements if it returns a scalar value or 
can be joined upon if it returns a result set.

--Scalar function
select dbo.ScalarFunctionName(@param1..)

--Table valued function
select * from dbo.TablevaluedfunctionName(@param1..)

SELECT a, LookupValue(b), c FROM customers;

Functions MUST return a value and cannot alter the data they receive as parameters. 
Functions are not allowed to change anything, must have at least one parameter(input only), 
and they must return a value. 

Stored procs do not require a parameter, can change database objects, and do not have to return a value.

You cannot call a SP from a function. Functions can be used in SELECT/WHERE/HAVING statements. Functions cannot 
use try-catch block or transactions. Functions are not DML.

SP's can have input/output parameters, can return zero/single/multiple parameters, can use transactions, can use
try-catch blocks. SP's cannot be used in SELECT/WHERE/HAVING statements.

Write a user-defined function when you want to compute and return a value for use in other SQL statements. 
Write a stored procedure when you want to group a possibly-complex set of SQL statements.
_______________

Aliasing columns becomes a must when you are going to manipulate data or use functions. Column aliases cannot be 
used in other parts of your query such as WHERE, HAVING and ON clauses. The only exception to this rule is the 
ORDER BY clause.
_______________

When refering to dates you don't want a query to return different results in different countries. For that reason 
it is recommended you always use the following format: 20131231, that is four digits for year, two for month and 
two for the day (yyyyMMdd). This format is not specific for any country, or culture neutral. 

The format 2013-12-31 (or yyyy-MM-dd) is an ISO standard and might seem to work well, but for historic reasons 
this format is NOT independent of culture for the DateTime and SmallDateTime data types.

* Record Section
Instead of datetime, use the time, date, datetime2 and datetimeoffset data types for new work(2008 or later). These 
types align with the SQL Standard and are more portable. time, datetime2 and datetimeoffset provide more seconds precision. 

SQL Server comes with the following data types for storing a date or a date/time value in the database:

date - format YYYY-MM-DD(2008)

time - format HH:MM:SS.0000000  TIME(3) will have milliseconds precision. TIME(7) is the highest and the default fractional second precision.
       Accuracy is 100 nanoseconds.(2008)		

datetime2 - format: YYYY-MM-DD HH:MM:SS (2008)

smalldatetime - format: YYYY-MM-DD HH:MM:SS (2000) Accurracy is always one minute.

datetimeoffset - Defines a date that is combined with a time of a day that has time zone awareness and is based on a 24-hour clock. (2008)
                 datetimeoffset(7) is the highest and the default fractional second precision. datetimeoffset provides time zone support 
				 for globally deployed applications. Datetimeoffset data type stores a UTC date internally. It also stores the time zone 
				 offset that was current at the time the timestamp was generated. With the same datetimeoffset value you can represent 
				 the value as a local time or easily convert it to a UTC time. SQL will do the right thing if you compare two datetimeoffset 
				 values, even if the values were captured from systems with very different time zone offsets. 

SQL Server Date Functions
The following table lists the most important built-in date functions in SQL Server:

Function	Description
GETDATE()	Returns the current date and time.
DATEPART(datepart,date)  
Returns an integer representing a single part of a date/time, such as year, month, week, weekday, day, dayofyear, hour, minute, etc.

DATENAME(datepart , date)
Returns a string(of an int) representing a single part of a date/time, such as year, month, week, weekday, day, dayofyear, hour, minute, etc.

DATEADD(datepart, numberOfIntervals, validDate)	
Used to add or subtract datetime. Its return a new datetime based on the added or subtracted 
interval. Datepart examples are: Year, month, day, quarter, hour, minute, second, dayofyear, weekday, week, millisecond, microsecond and nanosecond.

DATEDIFF()	Returns an integer value between two dates based on the datepart. Ex: DATEDIFF(datepart, startdate, enddate)

DAY()
MONTH()
YEAR()
CONVERT()	Displays date/time data in different formats

You can compare two dates easily if there is no time component involved.
SELECT * FROM Orders WHERE OrderDate='2008-11-11' will not work if the column contains a time element as well(2008-11-11 13:23:44).

**** Record Section End
_______________

Just like with a WHERE clause it is possible to order data by a column that is not in your SELECT list. There 
is one exception to this rule however. When using DISTINCT in your query you cannot order by columns that are 
not in the SELECT list. The reason is that when duplicates are removed the result rows do not necessarily map 
to the source rows in a one-to-one manner. ORDER BY items must appear in the select list if SELECT DISTINCT 
is specified.
_______________

Since OFFSET-FETCH is a SQL standard and TOP is not, it is recommended to use OFFSET-FETCH whenever possible. 
There are no WITH TIES or PERCENT equivalents for OFFSET-FETCH though. So if you need that functionality TOP 
would be your only choice.
_______________

Answer to the question "Why would you use a guid as a primary key?"
You would use guids as a key if you needed multiple databases synchronising via replication. Another reason 
to use guids is if you wanted to create rows on some remote client like a winforms app and then submit those 
to the server via web services etc.

If you do this I would strongly suggest that you make sure that you specify your own clustered index based 
on an auto incrementing int that is not unique. It can be a considerable overhead inserting rows into a table 
where the clustered index is a guid.

Update: Here is an example of how to set up a table like this:

CREATE TABLE [dbo].[myTable](
    [intId] [int] IDENTITY(1,1) NOT NULL,
    [realGuidId] [uniqueidentifier] NOT NULL,
    [someData] [varchar](50) NULL,
    CONSTRAINT [PK_myTable] UNIQUE NONCLUSTERED 
    (
        [realGuidId] ASC
    )
)

CREATE CLUSTERED INDEX [IX_myTable] ON [dbo].[myTable] 
(
[intId] ASC
)
You would insert into the table as normal e.g.:

INSERT INTO myTable VALUES(NEWID(), 'Some useful data goes here')
________________

When SET NOCOUNT is ON, the count (indicating the number of rows affected by a Transact-SQL statement) is not 
returned. When SET NOCOUNT is OFF, the count is returned. The @@ROWCOUNT function is updated even when SET NOCOUNT 
is ON. 

SET NOCOUNT ON eliminates the sending of DONE_IN_PROC messages to the client for each statement in a stored procedure. 
When using the utilities provided with Microsoft® SQL Server™ to execute queries, the results prevent "nn rows 
affected" from being displayed at the end Transact-SQL statements such as SELECT, INSERT, UPDATE, and DELETE.

For stored procedures that contain several statements that do not return much actual data, this can provide a 
significant performance boost because network traffic is greatly reduced.

* Record Section
One of these simple items that should be part of every stored procedure is SET NOCOUNT ON.  This one line of code, put 
at the top of a stored procedure turns off the messages that SQL Server sends back to the client after each T-SQL 
statement is executed.  This is performed for all SELECT, INSERT, UPDATE, and DELETE statements. Having this 
information is handy when you run a T-SQL statement in a query window, but when stored procedures are run there is no 
need for this information to be passed back to the client.

If you still need to get the number of rows affected by the T-SQL statement that is executing you can still 
use the @@ROWCOUNT option. By issuing a SET NOCOUNT ON this function (@@ROWCOUNT) still works and can still be used in 
your stored procedures to identify how many rows were affected by the statement.

Unless documented otherwise, all system stored procedures return a value of 0. This indicates success and a nonzero 
value indicates failure.

RETURN exits unconditionally from a query or procedure. RETURN is immediate and complete and can be used at any point to 
exit from a procedure, batch, or statement block. Statements that follow RETURN are not executed.

* End Record Section
______________

When using the INNER JOIN results from the main table that do not have at least one match in the join table are 
discarded. When the join table has multiple matches then rows from the main table are duplicated in the result.

In an INNER JOIN the predicate in the ON and WHERE clauses are interchangeable. The ON clause is always mandatory 
when working with INNER JOINS. That does not mean you have to check for equality between two rows. Could use 1 = 1.

With an OUTER JOIN we can preserve results that do not have a match. There are three kinds of OUTER JOINs, one 
that preserves rows from the main table(LEFT) if no match is found, one that preserves rows from the joined 
table(RIGHT) and one that preserves both(FULL).

Remember that with an INNER JOIN the ON clause and WHERE clause had the same filtering function. That is 
not the case for the OUTER JOIN. The OUTER JOIN discards rows on one side of the join, but keeps them on 
the other side. So the ON clause is used for matching purposes only. When adding an extra predicate to the ON 
clause it just means the right or left side of the join will have no values if the predicate results in FALSE 
(contrary to the INNER JOIN where the result was discarded as a whole).
_______________

The ORDER BY clause is invalid in views, inline functions, derived tables, subqueries, and common table 
expressions, unless TOP, OFFSET or FOR XML is also specified.
_______________

http://sql-mentor.blogspot.com/2008/11/using-t-sql-function-objectid.html
Here is a list of object types that you can pass to OBJECT_ID() function./*
AF = Aggregate function (CLR)
C = CHECK constraint
D = DEFAULT (constraint or stand-alone)
F = FOREIGN KEY constraint
PK = PRIMARY KEY constraint
P = SQL stored procedure
PC = Assembly (CLR) stored procedure
FN = SQL scalar function
FS = Assembly (CLR) scalar function
FT = Assembly (CLR) table-valued function
R = Rule (old-style, stand-alone)
RF = Replication-filter-procedure
S = System base table
SN = Synonym
SQ = Service queue
TA = Assembly (CLR) DML trigger
TR = SQL DML trigger
IF = SQL inline table-valued function
TF = SQL table-valued-function
U = Table (user-defined)
UQ = UNIQUE constraint
V = View
X = Extended stored procedure
IT = Internal table
_______________
Temporary_Tables_Variables

There are two types of temporary tables: local and global. Local temporary tables are visible only to their 
creators during the same connection to an instance of SQL Server as when the tables were first created or 
referenced. Local temporary tables are deleted after the user disconnects from the instance of SQL Server. 
Global temporary tables are visible to any user and any connection after they are created, and are deleted 
when all users that are referencing the table disconnect from the instance of SQL Server.

Table variables (DECLARE @t TABLE) are visible only to the connection that creates it, and are deleted when 
the batch or stored procedure ends.

Local temporary tables (CREATE TABLE #t) are visible only to the connection that creates it, and are deleted 
when the connection is closed. So, if you create a local temp table inside a sproc, and then try and access 
it outside that sproc, it won't exist

Global temporary tables (CREATE TABLE ##t) are visible to everyone, and are deleted when all connections 
that have referenced them have closed.

Tempdb permanent tables (USE tempdb CREATE TABLE t) are visible to everyone, and are deleted when the server 
is restarted.

Global and local stored procedures behave in terms of visibility and scope like global and local temporary 
tables. Create a temporary procedure by prefixing their names with # for local or ## for global.

---
There are a few differences between Temporary Tables (#tmp) and Table Variables (@tmp), although using tempdb isn't one of them. 
The table variable is NOT necessarily memory resident. Under memory pressure, the pages belonging to a table variable can be 
pushed out to tempdb.

As a rule of thumb, for small to medium volumes of data and simple usage scenarios you should use table variables. (This is an 
overly broad guideline with of course lots of exceptions.)

Some points to consider when choosing between them:

Temporary Tables are real tables so you can do things like CREATE INDEXes, etc. If you have large amounts of data for which accessing 
by index will be faster then temporary tables are a good option. Table variables can have indexes by using PRIMARY KEY or UNIQUE 
constraints. (If you want a non-unique index just include the primary key column as the last column in the unique constraint. If you 
don't have a unique column, you can use an identity column.) SQL 2014 has non-unique indexes too.

Table variables don't participate in transactions and SELECTs are implicitly with NOLOCK. The transaction behaviour can be very helpful, 
for instance if you want to ROLLBACK midway through a procedure then table variables populated during that transaction will still be populated!

Temp tables might result in stored procedures being recompiled, perhaps often. Table variables will not.
You can create a temp table using SELECT INTO, which can be quicker to write (good for ad-hoc querying) and may allow you to deal with changing 
datatypes over time, since you don't need to define your temp table structure upfront.

You can pass table variables back from functions, enabling you to encapsulate and reuse logic much easier (eg make a function to split a string 
into a table of values on some arbitrary delimiter).

Using Table Variables within user-defined functions enables those functions to be used more widely. If you're writing a function you should use 
table variables over temp tables unless there's a compelling need otherwise.

Both table variables and temp tables are stored in tempdb. But table variables (since 2005) default to the collation of the current database 
versus temp tables which take the default collation of tempdb (ref). This means you should be aware of collation issues if using temp tables 
and your db collation is different to tempdb's, causing problems if you want to compare data in the temp table with data in your database.
Global Temp Tables (##tmp) are another type of temp table available to all sessions and users
_______________

Difference between delete and truncate

DELETE
1. DELETE is a DML Command.
2. DELETE statement is executed using a row lock, each row in the table is locked for deletion.
3. We can specify filters in where clause
4. It deletes specified data if where condition exists.
5. Delete activates a trigger because the operation are logged individually.
6. Slower than truncate because, it keeps logs.
7. Rollback is possible.
 
TRUNCATE
1. TRUNCATE is a DDL command.
2. TRUNCATE TABLE always locks the table and page but not each row.
3. Cannot use Where Condition.
4. It Removes all the data.
5. TRUNCATE TABLE cannot activate a trigger because the operation does not log individual row deletions.
6. Faster in performance wise, because it doesn't keep any logs.
7. Rollback is not possible.
 
DELETE and TRUNCATE both can be rolled back when used with TRANSACTION.
 
If a Transaction is COMMITED, then we can not rollback the TRUNCATE command, but we can still rollback the 
DELETE command using the LOG files.

DELETE is a logged operation on a per row basis.  This means
that the deletion of each row gets logged and physically deleted.

You can DELETE any row that will not violate a constraint, while leaving the foreign key or any other 
contraint in place.

TRUNCATE is also a logged operation, but in a different way. 
TRUNCATE logs the deallocation of the data pages in which the data
exists.  The deallocation of data pages means that your data
rows still actually exist in the data pages, but the
extents have been marked as empty for reuse.  This is what
makes TRUNCATE a faster operation to perform over DELETE.

You cannot TRUNCATE a table that has any foreign key
constraints.  You will have to remove the contraints, TRUNCATE the
table, and reapply the contraints.

TRUNCATE will reset any identity columns to the default seed
value.  This means if you have a table with an identity column and
you have 264 rows with a seed value of 1, your last record will have
the value 264 (assuming you started with value 1) in its identity
columns.  After TRUNCATEing your table, when you insert a new
record into the empty table, the identity column will have a value of
1.  DELETE will not do this.  In the same scenario, if you
DELETEd your rows, when inserting a new row into the empty table, the
identity column will have a value of 265.

_______________

* Record Section
Identity columns can be used for generating key values. The identity property on a column guarantees the following:
	Each new value is generated based on the current seed & increment. Only one identity column can be created per table.
	Each new value for a particular transaction is different from other concurrent transactions on the table.

The identity property on a column does not guarantee the following:
	Uniqueness of the value – Uniqueness must be enforced by using a PRIMARY KEY or UNIQUE constraint or UNIQUE index.
	
	Consecutive values within a transaction – A transaction inserting multiple rows is not guaranteed to get consecutive 
	values for the rows because other concurrent inserts might occur on the table. If values must be consecutive then the 
	transaction should use an exclusive lock on the table or use the SERIALIZABLE isolation level.
	
	Consecutive values after server restart or other failures –SQL Server might cache identity values for performance 
	reasons and some of the assigned values can be lost during a database failure or server restart. This can result 
	in gaps in the identity value upon insert. If gaps are not acceptable then the application should use its own 
	mechanism to generate key values. Using a sequence generator with the NOCACHE option can limit the gaps to transactions 
	that are never committed.

	Reuse of values – For a given identity property with specific seed/increment, the identity values are not reused by the 
	engine. If a particular insert statement fails or if the insert statement is rolled back then the consumed identity values 
	are lost and will not be generated again. This can result in gaps when the subsequent identity values are generated.
_______________

A nonclustered index contains the index key values and row locators that point to the storage location of the 
table data. You can create multiple nonclustered indexes on a table or indexed view. Generally, nonclustered 
indexes should be designed to improve the performance of frequently used queries that are not covered by the 
clustered index. The query optimizer searches for a data value by searching the nonclustered index to find the 
location of the data value in the table and then retrieves the data directly from that location. 
_______________

http://technet.microsoft.com/en-us/library/ms179325(v=sql.105).aspx

Consider the characteristics of the database when designing nonclustered indexes.

Databases or tables with low update requirements, but large volumes of data can benefit from many nonclustered 
indexes to improve query performance. Consider creating filtered indexes for well-defined subsets of data to 
improve query performance, reduce index storage costs, and reduce index maintenance costs compared with full-table 
nonclustered indexes.

Decision Support System applications and databases that contain primarily read-only data can benefit from many 
nonclustered indexes. The query optimizer has more indexes to choose from to determine the fastest access method, 
and the low update characteristics of the database mean index maintenance will not impede performance.

Online Transaction Processing applications and databases that contain heavily updated tables should avoid 
over-indexing. Additionally, indexes should be narrow, that is, with as few columns as possible.

Large numbers of indexes on a table affect the performance of INSERT, UPDATE, DELETE, and MERGE statements because 
all indexes must be adjusted appropriately as data in the table changes.

Query Considerations
Before you create nonclustered indexes, you should understand how your data will be accessed. Consider using a 
nonclustered index for queries that:

Use JOIN or GROUP BY clauses.
  Create multiple nonclustered indexes on columns involved in join and grouping operations, and a clustered index 
  on any foreign key columns.

Queries that do not return large result sets.

Create filtered indexes to cover queries that return a well-defined subset of rows from a large table.

Contain columns frequently involved in search conditions of a query, such as WHERE clause, that return exact matches.
_______________

Create Indexes with Included Columns
You would use the INCLUDE to add one or more columns to the leaf level of a non-clustered index, if by doing so 
you can "cover" your queries. When an index contains all the columns referenced by a query it is typically 
referred to as covering the query.

CREATE NONCLUSTERED INDEX NC_EmpDep 
ON Employee(EmployeeID, DepartmentID)     -- Create non clustered index on these two columns.
INCLUDE (Lastname)                        -- This column would be in the select clause and not in ON or WHERE

If you have a non-clustered index on (EmployeeID, DepartmentID), once you find the employees for a given department 
you now have to do "bookmark lookup" to get the actual full employee record in order to get the lastname column. That 
can get pretty expensive in terms of performance if you find a lot of employees.

Nonkey columns can only be defined on nonclustered indexes.
All data types except text, ntext, and image can be used as nonkey columns.
Computed columns that are deterministic and either precise or imprecise can be nonkey columns.
_______________

Over Clause
Determines the partitioning and ordering of a rowset before the associated window function is applied. That is, 
the OVER clause defines a window or user-specified set of rows within a query result set. A window function then 
computes a value for each row in the window. You can use the OVER clause with functions to compute aggregated 
values such as moving averages, cumulative aggregates, running totals, or a top N per group results.
_______________

The basic rule to follow is Scans are bad, Seeks are good.

Index Scan
When SQL Server does a scan it loads the object which it wants to read from disk into memory, then reads through 
that object from top to bottom looking for the records that it needs.

Index Seek
When SQL Server does a seek it knows where in the index that the data is going to be, so it loads up the index 
from disk, goes directly to the part of the index that it needs and reads to where the data that it needs ends. 
This is a more efficient operation than a scan, as SQL already knows where the data is that it is looking for.

How can I modify an Execution Plan to use a Seek instead of a Scan?

When SQL Server is looking for your data probably one of the largest things which will make SQL Server switch 
from a seek to a scan is when some of the columns are you looking for are not included in the index you want it 
to use. Most often this will have SQL Server fall back to doing a clustered index scan, since the Clustered index 
contains all the columns in the table. This is one of the biggest reasons (in my opinion at least) that we now 
have the ability to INCLUDE columns in an index, without adding those columns to the indexed columns of the index. 
By including the additional columns in the index we increase the size of the index, but we allow SQL Server to 
read the index without having to go back to the clustered index, or to the table it self to get these values.
_______________

When there is an aggregate function in the select list, all other items in the select list must be included in
a group by clause. If they are not an error will occur when the query is run.
_______________

Remember that NULL can also be explained as UNKNOWN, so if a single NULL is returned from your subquery 
SQL Server returns no rows because it does not know if a value is or is not contained in the result. 

A value of NULL indicates that the value is unknown. A value of NULL is different from an empty or zero value. 
No two null values are equal. Comparisons between two null values, or between a NULL and any other value, return 
unknown because the value of each NULL is unknown.
_______________

The ANY operator works much like the IN operator, except in that you can use the >, <, >=, <=, = and <> operators 
to compare values. ANY returns true if at least one value returned  by the subquery makes the predicate true. 
Instead of ANY you can use SOME, which has the same meaning.

Unlike ANY, ALL looks at all results returned by a subquery and only returns TRUE if the comparison with all 
results makes the predicate true. 

EXISTS can be used like ANY and ALL, but returns true only if at least one record was returned by the subquery. 
It is pretty useful and you will probably use this more often. Let us say we want all customers that have 
placed at least one order.

SELECT *
FROM Sales.Customer AS c
WHERE EXISTS(SELECT *
        FROM Sales.SalesOrderHeader AS s
        WHERE s.CustomerID = c.CustomerID);

Notice that the EXISTS functions only returns TRUE or FALSE and not any columns. For that reason it does not 
matter what you put in the SELECT statement of your sub query. In fact, this is the only place where you can 
use SELECT * without worrying about it!
_______________
Tunning SQL queries
https://medium.com/tech-talk/f834f09bafaf
http://technet.microsoft.com/en-us/library/ms176005(v=sql.100).aspx

_______________

Distributed queries access data from multiple heterogeneous data sources, which can be stored on either the same or 
different computers. Microsoft® SQL Server™ 2000 supports distributed queries by using OLE DB, the Microsoft specification 
of an application programming interface (API) for universal data access. 

Distributed queries provide SQL Server users with access to:
   Distributed data stored in multiple instances of SQL Server.
   Heterogeneous data stored in various relational and non-relational data sources accessed using an OLE DB provider.
   
   Heterogeneous Data is data from any number of sources, largely unknown and unlimited, and in many varying formats. In essence, 
   it is a way to refer to data that is of an unknown format and/or content.

For a script to work as intended, regardless of the ANSI_NULLS database option or the setting of SET ANSI_NULLS, use IS NULL and 
IS NOT NULL in comparisons that might contain null values. SET ANSI_NULLS should be set to ON for executing distributed queries.
_______________

When SET ANSI_NULLS is ON, a SELECT statement that uses WHERE column_name = NULL returns zero rows even if there are null values in column_name. 
A SELECT statement that uses WHERE column_name <> NULL returns zero rows even if there are nonnull values in column_name. When SET ANSI_NULLS is 
OFF, the Equals (=) and Not Equal To (<>) comparison operators do not follow the ISO standard. A SELECT statement that uses WHERE column_name = NULL 
returns the rows that have null values in column_name. A SELECT statement that uses WHERE column_name <> NULL returns the rows that have nonnull 
values in the column. Also, a SELECT statement that uses WHERE column_name <> XYZ_value returns all rows that are not XYZ_value and that are not NULL.

_______________

Topic: nvarchar vs varchar and using N before a string 
varchar: Variable-length, non-Unicode character data. The database collation determines which code page the data is stored using.

nvarchar: Variable-length Unicode character data. Dependent on the database collation for comparisons.

An nvarchar column can store any Unicode data. A varchar column is restricted to an 8-bit codepage. Some people think that varchar should be used because it takes up less space. I believe this is not the correct answer. Codepage incompatabilities are a pain, and Unicode is the cure for codepage problems. With cheap disk and memory nowadays, there is really no reason to waste time mucking around with code pages anymore.

All modern operating systems and development platforms use Unicode internally. By using nvarchar rather than varchar, you can avoid doing encoding conversions every time you read from or write to the database. Conversions take time, and are prone to errors. And recovery from conversion errors is a non-trivial problem.

If you are interfacing with an application that uses only ASCII, I would still recommend using Unicode in the database. The OS and database collation algorithms will work better with Unicode. Unicode avoids conversion problems when interfacing with other systems. And you will be preparing for the future. And you can always validate that your data is restricted to 7-bit ASCII for whatever legacy system you're having to maintain, even while enjoying some of the benefits of full Unicode storage.

When passing a string in TSQL code, prefix it with an N to indicate string is in Unicode. This means that you are passing an NCHAR, NVARCHAR or NTEXT value opposed to CHAR, VARCHAR or TEXT.
_______________

For a standard view, the overhead of dynamically building the result set for each query that references a view can be 
significant for views that involve complex processing of large numbers of rows, such as aggregating lots of data, or 
joining many rows. If such views are frequently referenced in queries, you can improve performance by creating a unique 
clustered index on the view. When a unique clustered index is created on a view, the result set is stored in the database 
just like a table with a clustered index is stored. 

Another benefit of creating an index on a view is that the optimizer starts using the view index in queries that do 
not directly name the view in the FROM clause. Existing queries can benefit from the improved efficiency of retrieving 
data from the indexed view without having to be recoded. The main reason to use a view is to simplify a query or to 
standardize a way of accessing some data in a table. Generally speaking, you won't get a performance benefit. 

* Record Section
Indexed views can be a powerful tool, but they are not a 'free lunch' and we need to use them with care. Once we create an 
indexed view, every time we modify data in the underlying tables then not only must SQL Server maintain the index entries on 
those tables, but also the index entries on the view. This can affect write performance. In addition, they also have the potential 
to cause other issues. For example, if one or more of the base tables is subject to frequent updates, then, depending on the 
aggregations we perform in the indexed view, it is possible that we will increase lock contention on the view's index.
* Record Section
When we encapsulate complex multi-table query logic in a view, any application that needs that data is then able to issue a much 
simpler query against the view, rather than a complex multi-JOIN query against the underlying tables. Views bring other advantages 
too. We can grant users SELECT permissions on the view, rather than the underlying tables, and use the view to restrict the 
columns and rows that are accessible to the user. We can use views to aggregate data in a meaningful way
* Record Section
CREATE VIEW Sales.vCustomerOrders
WITH SCHEMABINDING
<Select statement here>
Note that the WITH SCHEMABINDING option is included here and is a requirement for creating an index on the view. This option 
stipulates that we cannot delete any of the base tables for the view, or ALTER any of the columns in those tables. In order to 
make one of these changes, we would have to drop the view, change the table, and then recreate the view (and any indexes on the view).

When you use the SchemaBinding keyword while creating a view or function you bind the structure of any underlying tables or views. So 
what does that mean? It means that as long as that schemabound object exists as a schemabound object you are limited in changes that 
can be made to the tables or views that it refers to. Only the columns referenced by the function or view are bound.

-- SchemaBinding example
CREATE SCHEMA Bound
GO
CREATE TABLE Bound.Table1 (Id Int, Col1 varchar(50), Col2 varchar(50))
CREATE TABLE Bound.Table2 (Id Int, Col1 varchar(50), Col2 varchar(50))
CREATE TABLE Bound.Table3 (Id Int, Col1 varchar(50), Col2 varchar(50))
GO
CREATE VIEW UnBoundView AS
SELECT Id, Col1, Col2 FROM Bound.Table1;
GO
CREATE VIEW BoundView WITH SCHEMABINDING AS
SELECT Table2.Id, Table2.Col1, Table2.Col2
FROM Bound.Table2
JOIN Bound.Table3
    ON Table2.Id = Table3.Id;
GO
ALTER TABLE Bound.Table1 DROP COLUMN Col2;
GO
-- Does not work, column and table are bound
ALTER TABLE Bound.Table2 DROP COLUMN Col2;
GO
-- This works because the column is not referenced in the bound view even though the table is.
ALTER TABLE Bound.Table2 ADD Col3 varchar(50);

-- Restrictions & factoids.
You can not change the collation of a database with schemabound objects.
You can not use SELECT * in a schemabound view.
You can not run sp_refreshview on a schemabound view. You do get a rather unhelpful error though.
You can make any change to the table that do not affect the structure of the bound columns.
You can find out if an object is schemabound by looking at the column is_schema_bound in sys.sql_modules or the system function 
OBJECTPROPERTY(object_id, ‘is_schema_bound’).
If you reference a view or function in a schemabound view or function then that view or function must also be schemabound.
Objects that are bound (tables/views) can not be dropped while a schemabound object references them.
_______________
TSQL_Performance_Tunning

Things that hurt TSQL performance
* Using the wrong data types
This is surprisingly simple in concept, but seems to be incredibly difficult in practice. Here you go… use the data type 
that is in your database. Use it in your parameters and in your variables. I know that SQL Server can implicitly convert 
from one to another. But when you get implicit conversions, or you have to put in explicit conversions, you’re performing 
a function on your columns. When you perform a function on your columns in any of the filtering scenarios, that’s a WHERE 
clause or JOIN criteria, you’re looking at generating table scans. You may have a perfectly good index, but because 
you’re doing a CAST on the column in order to compare to a character type that you passed in instead of a date, that 
index won’t get used.

* Using Functions in Comparisons within the ON or WHERE Clause
Speaking of functions, most of the functions that you run against your columns in WHERE and ON clauses will prevent the 
proper use of indexes. You will see slower performance since SQL Server has to perform scans against the data in order to 
take into account your function. When functions are used in the SELECT clause to return uppercase output, a substring or 
whatever, it doesn't affect performance that much, but when functions are used improperly in the WHERE clause these functions 
can cause major performance issues. Functions used in the WHERE clause forces SQL Server to do a table scan or index scan 
to get the correct results instead of doing an index seek if there is an index that can be used. The reason for this is 
that the function value has to be evaluated for each row of data to determine if it matches your criteria. 

* Indulging in Nested Views
Views which call views that join to views which are calling other views… A view is nothing but a query. But, because they 
appear to act like tables, people can come to think of them as tables. They’re not. What happens when you combine a view 
with a view and then nest them inside each other, etc., is that you’ve just created an incredibly complex execution plan. 
The optimizer will attempt to simplify things. It will try to come up with plans that don’t use every table referenced. 
But, it will only attempt to clean up your plans so many times. The more complex they get, the less likely that you’ll 
get a cleaned up plan. Then, performance becomes extremely problematic and inconsistent.

More info here:
https://www.simple-talk.com/sql/performance/the-seven-sins-against-tsql-performance/

One common problem that exists is the lack of indexes or incorrect indexes and therefore SQL Server has to process more 
data to find the records that meet the queries criteria.  These issues are known as Index Scans and Table Scans.

An index scan or table scan is when SQL Server has to scan the data or index pages to find the appropriate records.  A 
scan is the opposite of a seek, where a seek uses the index to pinpoint the records that are needed to satisfy the query.  
The reason you would want to find and fix your scans is because they generally require more I/O and also take longer to process.  
This is something you will notice with a database application that grows over time.

To find these issues you can start by running Profiler or setting up a server side trace and look for statements that have high 
read values.  Once you have identified the statements then you can look at the query plan to see if there are scans occurring.

Good short article to read and work through to learn more about tunning. Also contains some good links.
http://www.mssqltips.com/sqlservertutorial/277/index-scans-and-table-scans/

SQL Server's query optimizer uses distribution statistics to determine how it's going to satisfy your SQL query. These statistics 
represent the distribution of the data within a column, or columns. The Query Optimizer uses them to estimate how many rows will 
be returned from a query plan. With no statistics to show how the data is distributed, the optimizer has no way to compare 
the efficiency of different plans and so will be frequently forced to simply scan the table or index. Without statistics, it can’t 
possibly know if the column has the data you’re looking for without stepping through it. With statistics about the column, the 
optimizer can make much better choices about how it will access your data and use your indexes.

Distribution statistics are created automatically when you create an index. If you have enabled the automatic creation of statistics 
(the default setting of the AUTO_CREATE_STATISTICS database setting ) you’ll also get statistics created any time a column is 
referenced in a query as part of a filtering clause or JOIN criteria.

Data is measured two different ways within a single set of statistics, by density and by distribution.
Read This: https://www.simple-talk.com/sql/learn-sql-server/statistics-in-sql-server/

More info here:
https://www.simple-talk.com/sql/learn-sql-server/statistics-in-sql-server/
SQL Server’s query optimizer uses distribution statistics to determine how it’s going to satisfy your SQL query. These 
statistics represent the distribution of the data within a column, or columns. The Query Optimizer uses them to estimate 
how many rows will be returned from a query plan. With no statistics to show how the data is distributed, the optimizer 
has no way it can compare the efficiency of different plans and so will be frequently forced to simply scan the table or 
index. Without statistics, it can’t possibly know if the column has the data you’re looking for without stepping through it. 
With statistics about the column, the optimizer can make much better choices about how it will access your data and use your indexes.

Distribution statistics are created automatically when you create an index. If you have enabled the automatic creation of statistics 
(the default setting of the AUTO_CREATE_STATISTICS database setting ) you’ll also get statistics created any time a column is 
referenced in a query as part of a filtering clause or JOIN criteria.

Data is measured two different ways within a single set of statistics, by density and by distribution.

Database Engine Tuning Advisor in SQL Server
http://www.databasejournal.com/features/mssql/getting-starting-with-database-engine-tuning-advisor-in-sql-server-part-1.html

Database Engine Tuning Advisor is a utility that comes with SQL Server and can be used by both novice and experienced 
database administrators to get recommendations to improve the performance of SQL Server queries by making required 
physical structural changes. Based on your workload, Database Engine Tuning Advisor provides recommendations for best 
mix of indexes (clustered and non-clustered indexes) or indexed views, aligned or non-aligned partitions and required statistics.

Before you begin using Database Engine Tuning Advisor, you first need to collect the workload (a set of SQL Server queries that 
you want to optimize and tune). You can use direct queries, trace files, and trace tables generated from SQL Server Profiler as 
workload input when tuning databases. 

When you start analysis with Database Engine Tuning Advisor, it analyzes the provided workload and recommends to add, remove, or 
modify physical design structures in your databases like creating indexes and partitioning. It also recommends if additional 
statistics objects need to be created to support physical design structures. (As discussed in my earlier article, by default 
SQL Server automatically creates and maintains single column statistics on strategic columns; in addition to that, Database 
Engine Tuning Advisor recommends that you create multicolumn statistics based on your workload). Database Engine Tuning Advisor 
provides T-SQL scripts to quickly implement the recommendation and summary reports on the effects of implementing those 
recommendations for the provided workload.
_______________
Currency_
smallmoney has a range of - 214,748.3648 to 214,748.3647 (4bytes). money uses 8 bytes of storage.
The money and smallmoney data types are accurate to a ten-thousandth of the monetary units that they represent.

Currency or monetary data does not need to be enclosed in single quotation marks ( ' ). It is important to 
remember that while you can specify monetary values preceded by a currency symbol, SQL Server does not store 
any currency information associated with the symbol, it only stores the numeric value.
_______________

-- The following SQL creates a FOREIGN KEY on the "P_Id" column when the "Orders" table is created:
CREATE TABLE Orders
(
	O_Id int NOT NULL PRIMARY KEY,
	OrderNo int NOT NULL,
	P_Id int FOREIGN KEY REFERENCES Persons(P_Id)
)

-- To allow naming of a FOREIGN KEY constraint, and for defining a FOREIGN KEY constraint on multiple columns, use the following SQL syntax:
CREATE TABLE Orders
(
	O_Id int NOT NULL,
	OrderNo int NOT NULL,
	P_Id int,
	PRIMARY KEY (O_Id),
	CONSTRAINT fk_PerOrders FOREIGN KEY (P_Id)
	REFERENCES Persons(P_Id)
)

-- To drop a FOREIGN KEY constraint
ALTER TABLE Orders
DROP CONSTRAINT fk_PerOrders

-- Use this to add the named fk in InvoicedProducts table
ALTER TABLE dbo.InvoicedProducts
ADD CONSTRAINT FK_Products_ID FOREIGN KEY (referencing_column ) REFERENCES Products(referenced_column);
GO
_______________

Explicit conversions let you exercise more control over your data type conversions whenever you compare, 
combine, or move data from one database object to another. To support explicit conversions, SQL Server 
provides two important functions: CAST and CONVERT. The functions are similar in their ability to convert 
data. However, the CAST function conforms to ISO specifications, which makes it more portable. The CONVERT 
function, on the other hand, is specific to SQL Server, but it supports additional functionality that lets 
you better control the format of some types of data.
_______________

 The RAND math function returns a random float value from 0 through 1.  It can take an optional seed parameter, 
 which is an integer expression (tinyint, smallint or int) that gives the seed or start value. To use it, you 
 can simply do a simple SELECT, as follows:

SELECT RAND() AS [RandomNumber]

If you want to generate a random integer number, all you have to do is multiply it by the maximum value you 
want generated and then get rid of the decimal places.  One way of getting rid of the decimal places is by 
CASTing it to INT.

SELECT CAST(RAND() * 1000000 AS INT) AS [RandomNumber]

One thing to take note with the RAND function is that if the seed parameter is passed to it, the output 
will always be the same.

SELECT RAND(1) AS [RandomNumber]

If the RAND function is included in a SELECT statement on a table, the value returned for each row will 
be the same, as can be seen with the following example.

SELECT TOP 10 RAND() AS [RandomNumber], [CustomerID], [CompanyName] FROM [dbo].[Customers]

The NEWID system function can be used to generate a random numeric value as can be seen from the following SELECT statement.

SELECT ABS(CAST(CAST(NEWID() AS VARBINARY) AS INT)) AS [RandomNumber]
The NEWID system function returns a unique value of uniqueidentifier data type.  In order to convert this to 
an integer data type, it first has to be converted to VARBINARY then it can be converted to integer, as can 
be seen by the two CAST statements.  The resulting integer value can be positive and negative.  If you want 
it to be just a positive value, then we have to use the absolute value mathematical function ABS.
_______________

@ refers to the local variables you create with DECLARE statement, for example, declare @TotalCount int, @StartDate datetime
The parameters in functions and stored procedures also have @ in front of them. 
@@ refers to system defined functions, such as @@ROWCOUNT, @@Error, for example.
_______________

The COUNT(column_name) function returns the number of values (NULL values will not be counted) of the specified column:
SELECT COUNT(column_name) FROM table_name;

The COUNT(*) function returns the number of records in a table:

SELECT COUNT(*) FROM table_name;

The COUNT(DISTINCT column_name) function returns the number of distinct values of the specified column:

SELECT COUNT(DISTINCT column_name) FROM table_name;

LEN(string_expression)  Returns the number of characters of the specified string expression, excluding trailing blanks

DATALENGTH ( expression ) Returns the number of bytes used to represent any expression.
	USE AdventureWorks2012;
	GO
	SELECT length = DATALENGTH(Name), Name
	FROM Production.Product
	ORDER BY Name;
	GO

_______________

Common Table Expressions offer the same functionality as a view, but are ideal for one-off usages where you don't 
necessarily need a view defined for the system. Even when a CTE is not necessarily needed, it can improve readability. 
In Using Common Table Expressions, Microsoft offers the following four advantages of CTEs:

1) Create a recursive query.
2) Substitute for a view when the general use of a view is not required; that is, you do not have to store the definition in metadata.
3) Enable grouping by a column that is derived from a scalar subselect, or a function that is either not deterministic or has external access.
4) Reference the resulting table multiple times in the same statement.

Using a CTE offers the advantages of improved readability and ease in maintenance of complex queries. The query 
can be divided into separate, simple, logical building blocks. These simple blocks can then be used to build 
more complex, interim CTEs until the final result set is generated. 

Say, for instance, you have a query like this:
SELECT * FROM  (
        SELECT A.Address, E.Name, E.Age 
		From Address A
        Inner join Employee E on E.EID = A.EID) T
WHERE T.Age > 50
ORDER BY T.NAME

CTE's allow you to generate a temp result set beforehand and use it later when we actually bind the data into the output.
Rewriting the query using CTE expressions would look like:

With T(Address, Name, Age)  --Column names for Temporary table
AS
(
	SELECT A.Address, E.Name, E.Age 
	FROM Address A
	INNER JOIN EMP E ON E.EID = A.EID
)
SELECT * FROM T  --SELECT or USE CTE temporary result set
WHERE T.Age > 50
ORDER BY T.NAME

After you define your WITH clause with the CTEs, you can then reference the CTEs as you would refer any other table. 
However, you can refer a CTE only within the execution scope of the statement that immediately follows the WITH clause. 
After you’ve run your statement, the CTE result set is not available to other statements.
_______________

Using triggers is quite valid when their use is justified. For example, they have good value in auditing (keeping 
history of data) without requiring explicit procedural code with every CRUD command on every table.

Triggers give you control just before data is changed and just after the data is changed. This allows for:
	Auditing(keeping history of data) without requiring explicit procedural code with every CRUD command on every table.
	
	Validation and business security checking if so is desired. Because of this type of control, you can do tasks such 
	as column formatting before and after inserts into database.

If you were told to only use triggers if you really need to and opt to use stored procedures instead if possible,
the reasons for this may be:

Some functions that triggers used to do in the old days could now be performed in other ways such as updating totals and 
automatic calculation on a column. You don't see where the trigger is invoked by examining code alone without knowing they 
exist. You see their effect when you see the data changes and it is sometimes puzzling to figure out why the change 
occurred unless you know there is 1 trigger or more acting on the table(s). If you use several database controls such as 
CHECK, RI, Triggers on several tables, your transaction detailed flow becomes complex to understand and maintain. You will 
need to know exactly what happens when. Again, you will need good documentation for this.

A trigger is a special type of stored procedure that fires as a response of an event rather than be directly executed by 
the user. The event may be a change of data in a data column for example. Triggers have types. DDL Triggers and DML 
Triggers (of types: INSTEAD OF, For, and AFTER) Non-Trigger Stored procedures can reference any type of object, however, 
to reference a view, you must use INSTEAD OF triggers. In SQLServer, you can have any number on non-trigger stored 
procedures but only 1 INSTEAD OF trigger per table.

_______________
Using a SELECT statement with a simple CASE expression.
Within a SELECT statement, a simple CASE expression allows for only an equality check; no other comparisons are made. The 
following example uses the CASE expression to change the display of product line categories to make them more understandable.

USE AdventureWorks2012;
GO
SELECT   ProductNumber, Category =
      CASE ProductLine
         WHEN 'R' THEN 'Road'
         WHEN 'M' THEN 'Mountain'
         WHEN 'T' THEN 'Touring'
         WHEN 'S' THEN 'Other sale items'
         ELSE 'Not for sale'
      END,
   Name
FROM Production.Product
ORDER BY ProductNumber;
GO
_______________
Error_Handling

http://www.codeproject.com/Articles/38991/A-Closer-Look-Inside-RAISERROR-SQLServer
_______________

Can you create a clustered index on a column with duplicate values?

Yes and no. Yes, you can create a clustered index on key columns that contain duplicate values. No, the key columns cannot remain 
in a non-unique state. Let me explain. If you create a non-unique clustered index on a column, the database engine adds a four-byte 
integer (a uniquifier) to duplicate values to ensure their uniqueness and, subsequently, to provide a way to identify each row in 
the clustered table.

For example, you might decide to create a clustered index on the LastName column of a table that contains customer data. The column 
includes the values Franklin, Hancock, Washington, and Smith. You then insert the values Adams, Hancock, Smith, and Smith. Because 
the values in the key column must ultimately be unique, the database engine will modify the duplicates so that the values look 
something like this: Adams, Franklin, Hancock, Hancock1234, Washington, Smith, Smith4567, and Smith5678.

On the surface, this might seem an okay approach, but the integer increases the size of the key values, which could start becoming 
an issue if you have a lot of duplicate values and those values are being referenced by foreign keys and nonclustered indexes. For 
this reason, you should try to create unique clustered indexes whenever possible. If not possible, at least go for columns that have 
a high percentage of unique values.

_______________

Round() method
http://www.mssqltips.com/sqlservertip/1589/sql-server-rounding-functions--round-ceiling-and-floor/

DECLARE @value int
SET @value = 6

SELECT ROUND(@value, 1)  -- 6  - No rounding with no digits right of the decimal point
SELECT ROUND(@value, -1) -- 10 - Rounding up with digits on the left of the decimal point
SELECT ROUND(@value, 2)  -- 6  - No rounding with no digits right of the decimal point 
SELECT ROUND(@value, -2) -- 0  - Insufficient number of digits

SELECT ROUND(444,  1) -- 444  - No rounding with no digits right of the decimal point
SELECT ROUND(444, -1) -- 440  - Rounding down
SELECT ROUND(444,  2) -- 444  - No rounding with no digits right of the decimal point
SELECT ROUND(444, -2) -- 400  - Rounding down
SELECT ROUND(444,  3) -- 444  - No rounding with no digits right of the decimal point
SELECT ROUND(444, -3) -- 0    - Insufficient number of digits
SELECT ROUND(444,  4) -- 444  - No rounding with no digits right of the decimal point
SELECT ROUND(444, -4) -- 0    - Insufficient number of digits

DECLARE @value numeric(10,10)
SET @value = .5432167890
SELECT ROUND(@value, 1)  -- 0.5000000000 
SELECT ROUND(@value, 2)  -- 0.5400000000
SELECT ROUND(@value, 3)  -- 0.5430000000
SELECT ROUND(@value, 4)  -- 0.5432000000
SELECT ROUND(@value, 5)  -- 0.5432200000
SELECT ROUND(@value, 6)  -- 0.5432170000
SELECT ROUND(@value, 7)  -- 0.5432168000
SELECT ROUND(@value, 8)  -- 0.5432167900
SELECT ROUND(@value, 9)  -- 0.5432167890
SELECT ROUND(@value, 10) -- 0.5432167890
SELECT CEILING(@value)   -- 1
SELECT FLOOR(@value)     -- 0
_______________
varbinary

A varbinary column can store anything. To store a string in it, you'd have to cast it to varbinary:
declare @t table (id int identity, pwd varbinary(50))
insert into @t (pwd) values (cast('secret' as varbinary(50)))
But for a password, a varbinary column usually stores a hash of some kind. For example, a SHA1 hash using the HashBytes function:

insert into @t (pwd) values (HashBytes('sha1', 'secret'));
Storing a one-way hash instead of the real password is more secure. You can check if the password matches:

select * from @t where pwd = HashBytes('sha1', 'secret')
But there is no way you can retrieve the password by looking at the table. So only the end user knows his password, and not even the DBA can retrieve it.
_______________


_______________


_______________


_______________


_______________
